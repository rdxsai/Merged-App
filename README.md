# Question App

A comprehensive web application for managing Canvas LMS quiz questions with AI-powered feedback generation and an intelligent chat assistant using RAG (Retrieval-Augmented Generation).

## ğŸ—ï¸ Project Structure

This project follows Poetry best practices with a well-organized directory structure:

```
questionapp/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ question_app/          # Main application package
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py           # FastAPI application entry point
â”‚       â”œâ”€â”€ api/              # API endpoints and routers
â”‚       â”‚   â”œâ”€â”€ __init__.py   # API module exports
â”‚       â”‚   â”œâ”€â”€ canvas.py     # Canvas LMS integration endpoints
â”‚       â”‚   â”œâ”€â”€ questions.py  # Question CRUD endpoints
â”‚       â”‚   â”œâ”€â”€ chat.py       # RAG-based chat endpoints
â”‚       â”‚   â”œâ”€â”€ vector_store.py # Vector store operations endpoints
â”‚       â”‚   â”œâ”€â”€ objectives.py # Learning objectives management endpoints
â”‚       â”‚   â”œâ”€â”€ system_prompt.py  # System prompt management endpoints
â”‚       â”‚   â””â”€â”€ debug.py      # Debugging and testing endpoints
â”‚       â”œâ”€â”€ core/             # Core configuration and app setup
â”‚       â”‚   â”œâ”€â”€ __init__.py   # Core module exports
â”‚       â”‚   â”œâ”€â”€ config.py     # Centralized configuration management
â”‚       â”‚   â”œâ”€â”€ logging.py    # Centralized logging setup
â”‚       â”‚   â””â”€â”€ app.py        # FastAPI application setup
â”‚       â”œâ”€â”€ models/           # Pydantic models and data structures
â”‚       â”œâ”€â”€ services/         # Business logic and external integrations
â”‚       â””â”€â”€ utils/            # Utility functions and helpers
â”œâ”€â”€ scripts/                  # Development and build scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ build_docs.py
â”‚   â”œâ”€â”€ build_docs_simple.py
â”‚   â”œâ”€â”€ docs_and_serve.py
â”‚   â”œâ”€â”€ format_code.py
â”‚   â”œâ”€â”€ lint_code.py
â”‚   â”œâ”€â”€ run_tests.py
â”‚   â”œâ”€â”€ serve_docs.py
â”‚   â””â”€â”€ type_check.py
â”œâ”€â”€ config/                   # Configuration files
â”‚   â”œâ”€â”€ system_prompt.txt
â”‚   â”œâ”€â”€ chat_system_prompt.txt
â”‚   â””â”€â”€ chat_welcome_message.txt
â”œâ”€â”€ data/                     # Data files
â”‚   â”œâ”€â”€ quiz_questions.json
â”‚   â””â”€â”€ learning_objectives.json
â”œâ”€â”€ docs/                     # Documentation
â”œâ”€â”€ static/                   # Static web assets
â”œâ”€â”€ templates/                # HTML templates
â”œâ”€â”€ tests/                    # Test suite
â”œâ”€â”€ vector_store/             # Vector database storage
â”œâ”€â”€ .vscode/                  # VS Code configuration
â”‚   â”œâ”€â”€ settings.json         # Python interpreter, PYTHONPATH, formatting
â”‚   â”œâ”€â”€ tasks.json            # Development tasks (install, test, format, etc.)
â”‚   â”œâ”€â”€ launch.json           # Debug configurations
â”‚   â””â”€â”€ extensions.json       # Recommended VS Code extensions
â”œâ”€â”€ pyproject.toml           # Poetry configuration
â”œâ”€â”€ poetry.lock              # Dependency lock file
â””â”€â”€ README.md                # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Poetry (for dependency management)
- Canvas LMS API access
- Azure OpenAI API access (optional, for AI features)

### Installation

1. **Clone the repository:**

   ```bash
   git clone <repository-url>
   cd questionapp
   ```

2. **Install dependencies:**

   ```bash
   poetry install
   ```

3. **Set up environment variables:**
   Create a `.env` file in the project root with your configuration:

   ```env
   CANVAS_BASE_URL=https://canvas.vt.edu
   CANVAS_API_TOKEN=your_canvas_token
   COURSE_ID=your_course_id
   QUIZ_ID=your_quiz_id
   AZURE_OPENAI_ENDPOINT=your_azure_endpoint
   AZURE_OPENAI_DEPLOYMENT_ID=your_deployment_id
   AZURE_OPENAI_API_VERSION=2023-12-01-preview
   AZURE_OPENAI_SUBSCRIPTION_KEY=your_subscription_key
   OLLAMA_HOST=http://localhost:11434
   OLLAMA_EMBEDDING_MODEL=nomic-embed-text
   ```

   Optional: if you plan to use local embeddings via Ollama, start the Ollama
   service in a new terminal and then pull the embedding model in your current
   terminal:

   ```bash
   # New terminal - keep this running
   ollama serve

   # Original terminal
   ollama pull nomic-embed-text
   ```

4. **Run the application:**

   ```bash
   # Development mode
   poetry run dev

   # Production mode
   poetry run start
   ```

## ğŸ› ï¸ Development

### VS Code Configuration

This project includes comprehensive VS Code configuration for an optimal development experience:

- **`.vscode/settings.json`** - Python interpreter, PYTHONPATH, formatting, testing configuration
- **`.vscode/tasks.json`** - Pre-configured tasks for Poetry install, dev server, tests, lint, format, docs
- **`.vscode/launch.json`** - Debug configurations for API and tests
- **`.vscode/extensions.json`** - Recommended VS Code extensions

**Key Features:**

- âœ… **Type Safety**: 100% Pyright compliance, 80% Mypy compliance
- âœ… **Integrated Testing**: Pytest integration with Test Explorer
- âœ… **Debugging**: Full debugging support for FastAPI and tests
- âœ… **Code Quality**: Black formatting, isort imports, flake8 linting
- âœ… **Documentation**: Sphinx documentation building and serving

**Quick Start with VS Code:**

1. Open the project in VS Code
2. Install recommended extensions when prompted
3. Use Command Palette (`Cmd/Ctrl + Shift + P`) to access tasks:
   - `Tasks: Run Task` â†’ "Poetry: Install"
   - `Tasks: Run Task` â†’ "Run: Dev Server"
   - `Tasks: Run Task` â†’ "Test: Pytest"
   - `Tasks: Run Task` â†’ "Format: black+isort"

### Available Commands

The project includes several Poetry scripts for development tasks:

```bash
# Application
poetry run start          # Start the application
poetry run dev            # Start in development mode

# Testing
poetry run test           # Run all tests
poetry run test --type unit
poetry run test --type integration
poetry run test --type ai
poetry run test --type api

# Code Quality
poetry run format         # Format code with Black
poetry run lint           # Lint code with flake8
poetry run type-check     # Type checking with mypy

# Documentation
poetry run docs           # Build documentation
poetry run docs-simple    # Build docs without make
poetry run docs-serve     # Serve docs locally
```

### Development Workflow

1. **Code Formatting:**

   ```bash
   poetry run format
   ```

2. **Linting:**

   ```bash
   poetry run lint
   ```

3. **Type Checking:**

   ```bash
   poetry run type-check
   ```

4. **Running Tests:**

   ```bash
   poetry run test
   ```

5. **Building Documentation:**
   ```bash
   poetry run docs-serve
   ```

## ğŸ“š Documentation

Documentation is built using Sphinx and can be accessed by running:

```bash
poetry run docs-serve
```

This will build the documentation and serve it locally at `http://localhost:8000`.

## ğŸ”’ Type Safety

This project maintains high standards for type safety and code quality:

### Type Checking Results

- **Pyright**: 0 errors (100% compliance)
- **Mypy**: 3 remaining errors (80% improvement from 15 errors)
- **Remaining mypy errors**: Known limitations with complex nested logic (false positives)

### Type Safety Features

- âœ… **Comprehensive type annotations** across all modules
- âœ… **Pydantic models** for data validation
- âœ… **Type guards** for runtime type checking
- âœ… **Proper error handling** with typed exceptions
- âœ… **Integration with VS Code** for real-time type checking

### Recent Type Safety Improvements

- Fixed Canvas API string indexing issues
- Resolved Vector Store embeddings type compatibility
- Improved test exception handling with proper HTTPException types
- Added comprehensive type annotations to AI service
- Enhanced UploadFile handling in chat API

## ğŸ§ª Testing

The project includes comprehensive tests organized by type:

- **Unit Tests:** Test individual functions and classes
- **Integration Tests:** Test component interactions
- **AI Tests:** Test AI integration features
- **API Tests:** Test API endpoints

Run specific test types:

```bash
poetry run test --type unit
poetry run test --type integration
poetry run test --type ai
poetry run test --type api
```

## ğŸ”§ Configuration

Configuration files are stored in the `config/` directory:

- `system_prompt.txt` - Main system prompt for AI interactions
- `chat_system_prompt.txt` - Chat assistant system prompt
- `chat_welcome_message.txt` - Welcome message for chat interface

## ğŸ“ Data Management

Data files are stored in the `data/` directory:

- `quiz_questions.json` - Quiz questions data
- `learning_objectives.json` - Learning objectives data

## ğŸ—ï¸ Architecture

The application follows a modular architecture:

- **API Layer** (`src/question_app/api/`): FastAPI endpoints and routers
  - **Canvas API** (`canvas.py`): Canvas LMS integration endpoints
  - **Questions API** (`questions.py`): Question CRUD operations
  - **Chat API** (`chat.py`): RAG-based chat functionality
  - **Vector Store API** (`vector_store.py`): Vector store operations and semantic search
  - **Objectives API** (`objectives.py`): Learning objectives management
  - **Debug API** (`debug.py`): Debugging and testing endpoints
  - Additional API modules can be added for other functionality
- **Core Layer** (`src/question_app/core/`): Core application logic
- **Models Layer** (`src/question_app/models/`): Data models and schemas
- **Services Layer** (`src/question_app/services/`): Business logic and external integrations
- **Utils Layer** (`src/question_app/utils/`): Utility functions and helpers

## ğŸ”Œ API Structure

The API is organized into focused modules for better maintainability:

### Canvas Integration (`/api/*`)

- `GET /api/courses` - Get all available courses
- `GET /api/courses/{course_id}/quizzes` - Get all quizzes for a specific course
- `POST /api/configuration` - Update course and quiz configuration
- `POST /api/fetch-questions` - Fetch questions from Canvas API

### Question Management (`/*`)

- `GET /` - Main application page
- `GET /questions/{id}` - Question edit page
- `GET /questions/new` - New question creation page
- `POST /questions/{id}/generate-feedback` - Generate AI feedback
- And more...

### Chat Interface (`/chat/*`)

- `GET /chat/` - Chat interface page
- `POST /chat/message` - Process chat messages with RAG
- `GET /chat/system-prompt` - Chat system prompt management
- `GET /chat/welcome-message` - Welcome message management

### Vector Store Operations (`/vector-store/*`)

- `POST /vector-store/create` - Create vector store from questions
- `GET /vector-store/search` - Search vector store for relevant content
- `GET /vector-store/status` - Get vector store status
- `DELETE /vector-store/` - Delete vector store

### Learning Objectives Management (`/objectives/*`)

- `GET /objectives/` - Learning objectives management page
- `POST /objectives/` - Save learning objectives

### Debug and Testing (`/debug/*`)

- `GET /debug/question/{question_id}` - Inspect specific question details
- `GET /debug/config` - Check application configuration status
- `GET /debug/ollama-test` - Test Ollama connection and model availability

See `docs/api_structure.md` for detailed API documentation.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and linting
5. Submit a pull request

## ğŸ“„ Documentation

### Available Scripts

- `poetry run docs` - Build HTML documentation
- `poetry run docs-simple` - Build docs without make dependency
- `poetry run docs-serve` - Serve documentation locally

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¥ Authors

- **Bryce Kayanuma** - _Initial work_ - [BrycePK@vt.edu](mailto:BrycePK@vt.edu)
- **Robert Fentress** - [learn@vt.edu](mailto:learn@vt.edu)

## ğŸ¤– Setting Up RAG Pipeline for Conversations with Tutor

This section provides step-by-step instructions to set up the RAG (Retrieval-Augmented Generation) pipeline for conversational interactions with the Socratic tutor. This process involves multiple terminal instances, so follow each step carefully.

### Prerequisites

- Docker installed on your machine
- Poetry installed
- Ollama installed

### Setup Steps

1. **Start ChromaDB Service**

   Navigate to the root directory of the project (`questionapp`) and run:

   ```bash
   docker run -d \
     --name socratic_chroma \
     -p 8001:8000 \
     -v "$(pwd)/data:/chroma/data" \
     chromadb/chroma:0.4.24
   ```

   This starts the ChromaDB service in a Docker container with persistent storage mounted to your local `data` directory.

2. **Verify Container is Running**

   In the same terminal, verify the container is running:

   ```bash
   docker ps
   ```

3. **Start Ollama Service**

   Open a **new terminal instance** and start the Ollama service:

   ```bash
   ollama serve
   ```

   Keep this terminal running.

4. **Pull Embedding Model**

   Open another **new terminal instance** and pull the required embedding model (only needs to be done once):

   ```bash
   ollama pull nomic-embed-text
   ```

5. **Start the Application**

   Open another **new terminal instance** and start the backend service:

   ```bash
   poetry run dev
   ```

   Keep this terminal running and wait for the backend service to start completely.

6. **Create Vector Store**

   Open another **new terminal instance** and create the vector store:

   ```bash
   curl -X POST http://localhost:8080/vector-store/create
   ```

   You should see success messages in the terminal where the backend is running.
